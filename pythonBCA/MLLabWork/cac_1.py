# -*- coding: utf-8 -*-
"""CAC_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qKwEjSDfVIeqsvmIcIa8nP8q4XA_GHmL

###1.Compare and contrast – KNN, Naïve Bayes, Decision tree classification

#####Naive Bayes
1. Naive Bayes is a linear classifier while K-NN is not; It tends to be faster when applied to big data.   In comparison, k-nn is usually slower for large amounts of data, because of the calculations required for each new step in the process. If speed is important, choose Naive Bayes over K-NN.

2. In general, Naive Bayes is highly accurate when applied to big data. Don't discount K-NN when it comes to accuracy though; as the value of k in K-NN increases, the error rate decreases until it reaches that of the ideal Bayes (for k→∞).  

3. Naive Bayes offers you two hyperparameters to tune for smoothing: alpha and beta. A hyperparameter is a prior parameter that are tuned  on the training set to optimize it. In comparison, K-NN only has one option for tuning: the “k”, or number of neighbors.

#####KNN
1. If having conditional independence will highly negative affect classification, you'll want to choose K-NN over Naive Bayes. Naive Bayes can suffer from the zero probability problem; when a particular attribute's conditional probability equals zero, Naive Bayes will completely fail to produce a valid prediction. This could be fixed using a Laplacian estimator, but K-NN could end up being the easier choice.

2. Naive Bayes will only work if the decision boundary is linear, elliptic, or parabolic. Otherwise, choose K-NN.

#####Decision Tree
1. Of the three methods, decision trees are the easiest to explain and understand. Most people understand hierarchical trees, and the availability of a clear diagram can help you to communicate your results. Conversely, the underlying mathematics behind Bayes Theorem can be very challenging to understand for the layperson. K-NN meets somewhere in the middle; Theoretically, you could reduce the K-NN process to an intuitive graphic, even if the underlying mechanism is probably beyond a layperson's level of understanding.

2. Decision trees have easy to use features to identify the most significant dimensions, handle missing values, and deal with outliers.

###2.	K=1 case for KNN classification leads to over fitting. Demonstrate it using a suitable dataset and sample program
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data = pd.read_csv("/content/heart-disease.csv")

data.head()

X = data.drop(['target'],axis=1)
y=data['target']

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)

model_without_scaling = KNeighborsClassifier(n_neighbors=1)
model_without_scaling.fit(X_train,y_train)

print("TRAIN ACCURACY : {} %".format(model_without_scaling.score(X_train,y_train)*100))
print("TEST ACCURACY : {} %".format(model_without_scaling.score(X_test,y_test)*100))

"""As we can see that train accuracy is 100% so model is learning on the train data very well but when it comes to test data the accuracy drops to 62%. So it leads to overfitting.

###3.Explain the suitability of F measure as accuracy metric for class imbalanced data with an example
"""

data2 = pd.read_csv("/content/africa_recession.csv")

data2.head()

data2['growthbucket'].value_counts()

"""So this is a class imbalance data."""

X1 = data2.drop(['growthbucket'],axis=1)
y1 = data2['growthbucket']

X1_train,X1_test,y1_train,y1_test = train_test_split(X,y,test_size=0.2,random_state=42)

model_2 = KNeighborsClassifier(n_neighbors=5)
model_2.fit(X_train,y_train)

model_2.score(X1_test,y1_test)

from sklearn.metrics import confusion_matrix
confusion_matrix(y1_test, model_2.predict(X1_test))

from sklearn.metrics import classification_report
target_names=['0','1']
print(classification_report(y1_test, model_2.predict(X1_test), target_names=target_names))

"""In this case F1 measure comes handy"""

